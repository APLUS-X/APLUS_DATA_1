					Sklearn 与 TensorFlow 机器学习笔记


				               一·机器学习中常见4种学习方法、13种算法


1.监督学习
    输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。
    应用场景：分类问题和回归问题。
    常见算法：有逻辑回归（Logistic Regression）和反向传递神经网络（Back Propagation Neural Network）。

2.半监督学习
    输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。
    应用场景：分类和回归
    常见算法：一些对监督式学习算法的延伸，图论推理算法（Graph Inference），拉普拉斯支持向量机（Laplacian SVM.）等。

3.非监督学习
    数据并不被特别标识，学习模型是为了推断出数据的一些内在结构
    应用场景：关联规则的学习以及聚类等。
    常见算法：Apriori算法以及k-Means算法。
    
4.强化学习
    输入数据直接反馈到模型，模型必须对此立刻作出调整。
    应用场景：动态系统以及机器人控制等。
    常见算法：Q-Learning以及时间差学习（Temporal difference learning）。
    

常见算法：

1.回归算法
回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。
常见的回归算法包括：
最小二乘法（Ordinary Least Square），
逻辑回归（Logistic Regression），
逐步式回归（Stepwise Regression），
多元自适应回归样条（Multivariate Adaptive Regression Splines）
本地散点平滑估计（Locally Estimated Scatterplot Smoothing）。

2. 正则化方法
正则化方法是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整。
常见的算法包括：
Ridge Regression，
Least Absolute Shrinkage and Selection Operator（LASSO），
以及弹性网络（Elastic Net）。

3.决策树学习
决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常常用来解决分类和回归问题。
常见的算法包括：
分类及回归树（Classification And Regression Tree， CART）， 
ID3 (Iterative Dichotomiser 3)， 
C4.5， 
Chi-squared Automatic Interaction Detection(CHAID), Decision Stump, 
随机森林（Random Forest）， 
多元自适应回归样条（MARS）
梯度推进机（Gradient Boosting Machine， GBM）

4.基于实例的算法——“基于记忆的学习”
基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配
常见的算法：
 k-Nearest Neighbor(KNN), 
学习矢量量化（Learning Vector Quantization， LVQ），
自组织映射算法（Self-Organizing Map ， SOM）。

5.贝叶斯方法
贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。
常见算法：
朴素贝叶斯算法，
平均单依赖估计（Averaged One-Dependence Estimators， AODE），
Bayesian Belief Network（BBN）。

6.聚类算法
聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。
常见的聚类算法：
 k-Means算法
期望最大化算法（Expectation Maximization， EM）。

7.降低维度算法
降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。
常见的算法：
主成份分析（Principle Component Analysis， PCA），
偏最小二乘回归（Partial Least Square Regression，PLS），
Sammon映射，
多维尺度（Multi-Dimensional Scaling, MDS）, 
 投影追踪（Projection Pursuit）等。

8.关联规则学习
关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则
常见算法：
 Apriori算法和Eclat算法等。

9.遗传算法（genetic algorithm）
遗传算法模拟生物繁殖的突变、交换和达尔文的自然选择（在每一生态环境中适者生存）。它把问题可能的解编码为一个向量，称为个体，向量的每一个元素称为基因，并利用目标函数（相应于自然选择标准）对群体（个体的集合）中的每一个个体进行评价，根据评价值（适应度）对个体进行选择、交换、变异等遗传操作，从而得到新的群体。遗传算法适用于非常复杂和困难的环境，比如，带有大量噪声和无关数据、事物不断更新、问题目标不能明显和精确地定义，以及通过很长的执行过程才能确定当前行为的价值等

10.人工神经网络
人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。
重要的人工神经网络算法包括：
感知器神经网络（Perceptron Neural Network）, 
反向传递（Back Propagation）， 
Hopfield网络，
自组织映射（Self-Organizing Map, SOM）。

11.深度学习
很多深度学习的算法是半监督式学习算法，用来处理存在少量未标识数据的大数据集。
常见的深度学习算法包括：
受限波尔兹曼机（Restricted Boltzmann Machine， RBN），
 Deep Belief Networks（DBN），
卷积网络（Convolutional Network）, 
堆栈式自动编码器（Stacked Auto-encoders）。

12.基于核的算法
 基于核的算法把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易的解决。
常见的基于核的算法包括：
支持向量机（Support Vector Machine， SVM）， 
径向基函数（Radial Basis Function ，RBF)， 
以及线性判别分析（Linear Discriminate Analysis ，LDA)等。

13.集成算法
集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。
常见的算法包括：
Boosting， 
Bootstrapped Aggregation（Bagging）， 
AdaBoost，
堆叠泛化（Stacked Generalization， Blending），
梯度推进机（Gradient Boosting Machine, GBM），
随机森林（Random Forest），
GBDT（Gradient Boosting Decision Tree）。





					               二·机器学习概念

1.Scikit-learn 非常易于使用，并且实现了许多有效的机器学习算法，因此它为学习机器学习提供了一个很好的切入点。

2.TensorFlow 是使用数据流图进行分布式数值计算的更复杂的库。它通过在潜在的数千个多 GPU 服务器上分布式计算，可以高效地训练和运行非常大的神经网络。TensorFlow 是被 Google 创造的，支持其大型机器学习应用程序。于 2015年11月开源。

准备条件：微积分，线性代数，概率和统计学

第一部分，机器学习的基础知识，涵盖以下主题：
	什么是机器学习？它被试图用来解决什么问题？机器	学习系统的主要类别和基本概念是什么？
	典型的机器学习项目中的主要步骤。
	通过拟合数据来学习模型。
	优化成本函数（cost function）。
	处理，清洗和准备数据。
	选择和设计特征。
	使用交叉验证选择一个模型并调整超参数。
	机器学习的主要挑战，特别是欠拟合和过拟合（偏差和方差权衡）。
	对训练数据进行降维以对抗 the curse of dimensionality（维度诅咒）
	最常见的学习算法：线性和多项式回归， Logistic 回归，k-最近邻，支持向量机，决策树，随机森林和集成方法。

第二部分，神经网络和深度学习，包括以下主题：
	什么是神经网络？它们有啥优势？
	使用 TensorFlow 构建和训练神经网络。
	最重要的神经网络架构：前馈神经网络，卷积网络，递归网络，长期短期记忆网络（LSTM）和自动编码器。
	训练深度神经网络的技巧。
	对于大数据集缩放神经网络。
	强化学习。

计算机程序利用经验 A 学习任务 B，性能是 C，如果针对任务 B 的性能 C 随着经验 A 不断增长，（通过训练集，不断识别特征，不断建模，最后形成有效的模型）则称为机器学习。

机器学习的基本思路
1.把现实生活中的问题抽象成数学模型，并且很清楚模型中不同参数的作用
2.利用数学方法对这个数学模型进行求解，从而解决现实生活中的问题
3.评估这个数学模型，是否真正的解决了现实生活中的问题，解决的如何？

机器学习实现原理：
假如我们正在教小朋友识字（一、二、三）。我们首先会拿出3张卡片，然后便让小朋友看卡片，一边说“一条横线的是一、两条横线的是二、三条横线的是三”。
上面提到的认字的卡片在机器学习中叫——训练集
上面提到的“一条横线，两条横线”这种区分不同汉字的属性叫——特征
小朋友不断学习的过程叫——建模
学会了识字后总结出来的规律叫——模型

机器学习在实际操作层面一共分为7步：
收集数据——数据准备——选择一个模型——训练——评估——参数调整——预测（开始使用）

机器学习的另一个优点是善于处理对于传统方法太复杂或是没有已知算法的问题

使用机器学习方法挖掘大量数据，可以发现并不显著的规律。这称作数据挖掘。

总结一下，机器学习善于：
	需要进行大量手工调整或需要拥有长串规则才能解决的问题：机器学习算法通常可以简化代码、提高性能。
	问题复杂，传统方法难以解决：最好的机器学习方法可以找到解决方案。
	环境有波动：机器学习算法可以适应新数据。
	洞察复杂问题和大量数据。

机器学习系统的类型：
机器学习有多种类型，可以根据如下规则进行分类：
	是否在人类监督下进行训练（监督，非监督，半监督和强化学习）
	是否可以动态渐进学习（在线学习 vs 批量学习）
	它们是否只是通过简单地比较新的数据点和已知的数据点，或者在训练数据中进行模式识别，以建立一个预测模型，就像科学家所做的那样（基于实例学习 vs 基于模型学习）
	规则并不仅限于以上的，你可以将他们进行组合。例如，一个先进的垃圾邮件过滤器可以使用神经网络模型动态进行学习，用垃圾邮件和普通邮件进行训练。这就让它成了一个在线、基于模型、监督学习系统。

机器学习可以根据训练时监督的量和类型进行分类。主要有四类：监督学习、非监督学习、半监督学习和强化学习。

监督学习
	分类算法：
	垃圾邮件过滤器就是一个很好的例子：用许多带有归类（垃圾邮件或普通邮件）的邮件样本进行训练，过滤器必须还能对新邮件进行分类。
	预测目标数值：
	给出一些特征（里程数、车龄、品牌等等）称作预测值，来预测一辆汽车的价格。这类任务称作回归。要训练这个系统，你需要给出大量汽车样本，包括它们的预测值和标签（即，它们的价格）。
	一些回归算法也可以用来进行分类，反之亦然。例如，逻辑回归通常用来进行分类，它可以生成一个归属某一类的可能性的值
监督学习算法：
	K近邻算法，线性回归，逻辑回归，支持向量机（SVM），决策树和随机森林，神经网络
监督学习是指我们给算法一个数据集，并且给定正确答案。机器通过数据来学习正确答案的计算方法。
举个栗子：
我们准备了一大堆猫和狗的照片，我们想让机器学会如何识别猫和狗。当我们使用监督学习的时候，我们需要给这些照片打上标签。
我们给照片打的标签就是“正确答案”，机器通过大量学习，就可以学会在新照片中认出猫和狗。
这种通过大量人工打标签来帮助机器学习的方式就是监督学习。这种学习方式效果非常好，但是成本也非常高。

非监督学习：
	训练数据是没有加标签的，系统在没有老师的条件下进行学习。

非监督学习算法：
	聚类
	K 均值
	层次聚类分析（Hierarchical Cluster Analysis，HCA）
	期望最大值
	可视化和降维
	主成分分析（Principal Component Analysis，PCA）
	核主成分分析
	局部线性嵌入（Locally-Linear Embedding，LLE）
	t-分布邻域嵌入算法（t-distributed Stochastic Neighbor Embedding，t-SNE）
	关联性规则学习
	Apriori 算法
	Eclat 算法
非监督学习中，给定的数据集没有“正确答案”，所有的数据都是一样的。无监督学习的任务是从给定的数据集中，挖掘出潜在的结构。
举个栗子：
我们把一堆猫和狗的照片给机器，不给这些照片打任何标签，但是我们希望机器能够将这些照片分分类。
通过学习，机器会把这些照片分为2类，一类都是猫的照片，一类都是狗的照片。虽然跟上面的监督学习看上去结果差不多，但是有着本质的差别：
非监督学习中，虽然照片分为了猫和狗，但是机器并不知道哪个是猫，哪个是狗。对于机器来说，相当于分成了 A、B 两类。


聚类

	可视化算法也是极佳的非监督学习案例：给算法大量复杂的且不加标签的数据，算法输出数据的2D或3D图像。算法会试图保留数据的结构（即尝试保留输入的独立聚类，避免在图像中重叠），这样就可以明白数据是如何组织起来的，也许还能发现隐藏的规律。

降维，
	降维的目的是简化数据、但是不能失去大部分信息。做法之一是合并若干相关的特征。例如，汽车的里程数与车龄高度相关，降维算法就会将它们合并成一个，表示汽车的磨损。这叫做特征提取。

异常检测（anomaly detection）
	 例如，检测异常的信用卡转账以防欺诈，检测制造缺陷，或者在训练之前自动从训练数据集去除异常值。异常检测的系统使用正常值训练的，当它碰到一个新实例，它可以判断这个新实例是像正常值还是异常值

关联规则学习，
	它的目标是挖掘大量数据以发现属性间有趣的关系。例如，假设你拥有一个超市。在销售日志上运行关联规则，可能发现买了烧烤酱和薯片的人也会买牛排。因此，你可以将这些商品放在一起。

半监督学习
	一些算法可以处理部分带标签的训练数据，通常是大量不带标签数据加上小部分带标签数据。这称作半监督学习
	多数半监督学习算法是非监督和监督算法的结合。例如，深度信念网络（deep belief networks）是基于被称为互相叠加的受限玻尔兹曼机（restricted Boltzmann machines，RBM）的非监督组件。RBM 是先用非监督方法进行训练，再用监督学习方法进行整个系统微调

强化学习：
   许多机器人运行强化学习算法以学习如何行走。DeepMind 的 AlphaGo 也是强化学习的例子：它在 2016 年三月击败了世界围棋冠军李世石（译者注：2017 年五月，AlphaGo 又击败了世界排名第一的柯洁）。它是通过分析数百万盘棋局学习制胜策略，然后自己和自己下棋。要注意，在比赛中机器学习是关闭的；AlphaGo 只是使用它学会的策略。

强化学习更接近生物学习的本质，因此有望获得更高的智能。它关注的是智能体如何在环境中采取一系列行为，从而获得最大的累积回报。通过强化学习，一个智能体应该知道在什么状态下应该采取什么行为。

最典型的场景就是打游戏。


批量学习
	在批量学习中，系统不能进行持续学习：必须用所有可用数据进行训练。这通常会占用大量时间和计算资源，所以一般是线下做的。首先是进行训练，然后部署在生产环境且停止学习，它只是使用已经学到的策略。这称为离线学习。

在线学习
	在在线学习中，是用数据实例持续地进行训练，可以一次一个或一次几个实例（称为小批量）。每个学习步骤都很快且廉价，所以系统可以动态地学习到达的新数据

在线学习系统的一个重要参数是，它们可以多快地适应数据的改变：
	这被称为学习速率。如果你设定一个高学习速率，系统就可以快速适应新数据，但是也会快速忘记老数据（你可不想让垃圾邮件过滤器只标记最新的垃圾邮件种类）。相反的，如果你设定的学习速率低，系统的惰性就会强：即，它学的更慢，但对新数据中的噪声或没有代表性的数据点结果不那么敏感。

基于实例 vs 基于模型学习
基于实例：
	也许最简单的学习形式就是用记忆学习。如果用这种方法做一个垃圾邮件检测器，只需标记所有和用户标记的垃圾邮件相同的邮件 —— 这个方法不差，但肯定不是最好的。
	不仅能标记和已知的垃圾邮件相同的邮件，你的垃圾邮件过滤器也要能标记类似垃圾邮件的邮件。这就需要测量两封邮件的相似性。一个（简单的）相似度测量方法是统计两封邮件包含的相同单词的数量。如果一封邮件含有许多垃圾邮件中的词，就会被标记为垃圾邮件
基于模型学习：
	从样本集进行归纳的方法是建立这些样本的模型，然后使用这个模型进行预测。这称作基于模型学习
	
	对于线性回归问题，人们一般是用代价函数测量线性模型的预测值和训练样本的距离差，目标是使距离差最小。

使用 Scikit-Learn 训练并运行线性模型。
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import sklearn
# 加载数据
oecd_bli = pd.read_csv("oecd_bli_2015.csv", thousands=',')
gdp_per_capita = pd.read_csv("gdp_per_capita.csv",thousands=',',delimiter='\t',
                             encoding='latin1', na_values="n/a")
# 准备数据
country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)
X = np.c_[country_stats["GDP per capita"]]
y = np.c_[country_stats["Life satisfaction"]]
# 可视化数据
country_stats.plot(kind='scatter', x="GDP per capita", y='Life satisfaction')
plt.show()
# 选择线性模型
lin_reg_model = sklearn.linear_model.LinearRegression()
# 训练模型
lin_reg_model.fit(X, y)
# 对塞浦路斯进行预测
X_new = [[22587]]  # 塞浦路斯的人均GDP
print(lin_reg_model.predict(X_new)) # outputs [[ 5.96242338]]

获取更多更好的训练数据，或选择一个更好的模型
1.研究数据
2.选择模型
3.用训练数据进行训练（即，学习算法搜寻模型参数值，使代价函数最小）
4.最后，使用模型对新案例进行预测（这称作推断），但愿这个模型推广效果不差

因为你的主要任务是选择一个学习算法并用一些数据进行训练，会导致错误的两件事就是“错误的算法”和“错误的数据”。
训练数据量不足——没有代表性的训练数据——低质量数据——不相关的特征——过拟合训练数据——欠拟合训练数据

    训练数据量不足——机器学习需要大量的数据作为支持，才能使机器学习算法正常工作，即使非常简单的问题，也需要数千的样本，但是，应该注意到，小型和中型的数据集仍然是非常常见的，获得额外的训练数据并不总是轻易和廉价的，所以不要抛弃算法。

    没有代表性的训练数据——使用具有代表性的训练集对于推广到新案例是非常重要的。但是做起来比说起来要难：如果样本太小，就会有样本噪声（即，会有一定概率包含没有代表性的数据），但是即使是非常大的样本也可能没有代表性，如果取样方法错误的话。这叫做样本偏差。

    低质量数据——如果训练集中的错误、异常值和噪声（错误测量引入的）太多，系统检测出潜在规律的难度就会变大，性能就会降低。

    不相关的特征——进来的是垃圾，出去的也是垃圾。你的系统只有在训练数据包含足够相关特征、非相关特征不多的情况下，才能进行学习。机器学习项目成功的关键之一是用好的特征进行训练。这个过程称作特征工程
1.特征选择：在所有存在的特征中选取最有用的特征进行训练。
2.特征提取：组合存在的特征，生成一个更有用的特征（如前面看到的，可以使用降维算法）。
3.收集新数据创建新特征。

过拟合训练数据——模型在训练数据上表现很好，但是推广效果不好。（如果你在外国游玩，当地的出租车司机多收了你的钱。你可能会说这个国家所有的出租车司机都是小偷。）复杂的模型，比如深度神经网络，可以检测数据中的细微规律，但是如果训练集有噪声，或者训练集太小（太小会引入样本噪声），模型就会去检测噪声本身的规律。很明显，这些规律不能推广到新实例。限定一个模型以让它更简单，降低过拟合的风险被称作正则化（regularization），
过拟合发生在相对于训练数据的量和噪声，模型过于复杂的情况。可能的解决方案有：
1.简化模型，可以通过选择一个参数更少的模型（比如使用线性模型，而不是高阶多项式模型）、减少训练数据的属性数、或限制一下模型
2.收集更多的训练数据
3.减小训练数据的噪声（比如，修改数据错误和去除异常值）

欠拟合训练数据——欠拟合是和过拟合相对的：当你的模型过于简单时就会发生。生活满意度的线性模型倾向于欠拟合；现实要比这个模型复杂的多，所以预测很难准确，即使在训练样本上也很难准确。
解决这个问题的选项包括：
1.选择一个更强大的模型，带有更多参数
2.用更好的特征训练学习算法（特征工程）
3.减小对模型的限制（比如，减小正则化超参数）

总结
1.机器学习是让机器通过学习数据对某些任务做得更好，而不使用确定的代码规则。
2.有许多不同类型的机器学习系统：监督或非监督，批量或在线，基于实例或基于模型，等等。
3.在机器学习项目中，我们从训练集中收集数据，然后对学习算法进行训练。如果算法是基于模型的，就调节一些参数，让模型拟合到训练集（即，对训练集本身作出好的预测），然后希望它对新样本也能有好预测。如果算法是基于实例的，就是用记忆学习样本，然后用相似度推广到新实例。
4.如果训练集太小、数据没有代表性、含有噪声、或掺有不相关的特征（垃圾进，垃圾出），系统的性能不会好。最后，模型不能太简单（会发生欠拟合）或太复杂（会发生过拟合）。

一个模型推广到新样本的效果，唯一的办法就是真正的进行试验。
训练集和测试集。用训练集进行训练，用测试集进行测试。对新样本的错误率称作推广错误（或样本外错误），通过模型对测试集的评估，你可以预估这个错误。
一般使用 80% 的数据进行训练，保留20%用于测试。

假设你发现最佳的超参数的推广错误率最低，比如只有 5%。然后就选用这个模型作为生产环境，但是实际中性能不佳，误差率达到了 15%。发生了什么呢？
_(问题原因)，你在测试集上多次测量了推广误差率，调整了模型和超参数，以使模型最适合这个集合。这意味着模型对新数据的性能不会高。
_(解决方案)，再保留一个集合，称作验证集合。用训练集和多个超参数训练多个模型，选择在验证集上有最佳性能的模型和超参数。当你对模型满意时，用测试集再做最后一次测试，以得到推广误差率的预估。
为了避免“浪费”过多训练数据在验证集上，通常的办法是使用交叉验证：训练集分成互补的子集，每个模型用不同的子集训练，再用剩下的子集验证。一旦确定模型类型和超参数，最终的模型使用这些超参数和全部的训练集进行训练，用测试集得到推广误差率。

模型是观察的简化版本。简化意味着舍弃无法进行推广的表面细节。但是，要确定舍弃什么数据、保留什么数据，必须要做假设。
如果完全不对数据做假设，就没有理由选择一个模型而不选另一个。对于一些数据集，最佳模型是线性模型，而对其它数据集是神经网络。没有一个模型可以保证效果更好。确信的唯一方法就是测试所有的模型。因为这是不可能的，实际中就必须要做一些对数据合理的假设，只评估几个合理的模型。例如，对于简单任务，你可能是用不同程度的正则化评估线性模型，对于复杂问题，你可能要评估几个神经网络模型。


1.如何定义机器学习？
   机器学习是为了构建可根据数据学习的系统。学习意味着在某项任务中取得更好的成绩, 给出一些性能指标。
   计算机程序利用经验 A 学习任务 B，性能是 C，如果针对任务 B 的性能 C 随着经验 A 不断增长

2.机器学习可以解决的四类问题？
   适合解决那些我们没有算法方案来解决的复杂问题，可以替换长时间的手动调整规则列表，构建适应波动环境的系统,，最后帮助人类学习（如：数据挖掘）

3.什么是带标签的训练集？
   一个训练集包含了每个实例所需的解决方案（也可理解为预期的结果）

4.最常见的两个监督任务是什么？
   回归和分类

5.指出四个常见的非监督任务？
   聚类，可视化，降维，关联规则学习

6.要让一个机器人能在各种未知地形行走，你会采用什么机器学习算法？
   强化学习可能会表现得最好，

7.要对你的顾客进行分组，你会采用哪类算法？
   如果你不知道如果定义分组，则可以通过聚类算法（无监督学习）将客户细分为类似客户的集群。但是，当你知道你想要哪些组，则可以将每个分组中的示例提供给分类算法中（监督学习），然后它会将你的顾客归类到这些群组中。

8.垃圾邮件检测是监督学习问题，还是非监督学习问题？
   监督学习问题

9.什么是在线学习系统？
   一个在线学习系统可以逐步学习，而不是分批学习系统。这使得它能够快速适应不断变化的数据和自治系统， 以及对大量数据进行训练。

10.什么是核外学习？
   核外（out-of-core）算法可以处理大量无法容纳在计算机主内存中的数据。一个核外学习算法将数据分割成小数据块，并使用在线学习技术从这些小数据块中学习。

11.什么学习算法是用相似度做预测？
   基于实例的学习系统通过心学训练数据。然后，当给定一个新实例时，它使用相似性度量来查找最相似的学习实例，并使用它们进行预测。

12.模型参数和学习算法的超参数的区别是什么？
   模型有一个或多个模型参数来决定当定一个新实例将预测什么 (如：线性模型的斜率)。学习算法试图找到这些参数的最佳值，使模型能够很好地推广到新的实例。超参数是学习算法本身的参数，而不是模型 (如：应用的正则化量)。

13.基于模型学习的算法搜寻的是什么？最成功的策略是什么？基于模型学习如何做预测？
   基于模型的学习算法搜索模型参数的最优值，使模型能够很好地推广到新实例。我们通常通过最小化成本函数来训练这样的系统，它能测量系统在对训练数据进行预测时的糟糕程度，再加上在模型正规化时对模型复杂度的惩罚。为了进行预测, 我们利用学习算法找到的参数值，将新实例的特征引入到模型的预测函数中。

14.机器学习的四个主要挑战是什么？
   缺乏数据——数据质量不高，缺乏代表性——无特点，过于简单的模型欠拟合数据，以及过度复杂的模型过度拟合数据。

15.如果模型在训练集上表现好，但推广到新实例表现差，问题是什么？给出三个可能的解决方案。
   这个模型可能过度拟合了训练数据（或者是我们得到了幸运的训练数据）。过度拟合可能的解决办法就是获取更多的数据，简化模型（选择更简单的算法，减少使用的参数或特征的数量，或使模型更加规范化），或对训练数据降噪。

16.什么是测试集，为什么要使用它？
   模型在生产中启动之前，使用测试集来估计模型对新实例产生的泛化误差。

17.验证集的目的是什么？
   验证集用于比较模型。它将使我们尽可能得选择最佳模型和优化超参数。

18.如果用测试集调节超参数，会发生什么？
   如果你使用测试集来调节超参数，你冒着过度拟合测试集得风险，并且你度量的泛化误差将是乐观的 (你可能会启动一个性能比预期更差的模型)。

19.什么是交叉验证，为什么它比验证集好？
   交叉验证是一项用来比较模型的技术 (用于模型选择和超参数优化)，而无需单独的验证集。这将节省宝贵的训练数据。






















